application {
  cluster {
    # The system name defined for the creation of the akka cluster
    # The cluster seed-nodes need to use this name and the Main function
    # should create the ActorSystem with this name
    name = "ai-beyond-compute-cluster"
    name = ${?CLUSTER.NAME}

    seed {
      host = "127.0.0.1"
      host = ${?AKKA.SEED.HOST}

      port = 2551
      port = ${?AKKA.SEED.PORT}
    }
  }

  http {
    # Port to bind the HTTP API
    port = 5000
    port = ${?HTTP.PORT}

    # The host address to bind the HTTP API
    host = "0.0.0.0"
    host = ${?HTTP.HOST}

    # Request time out in seconds
    request-timeout = 20 seconds
    request-timeout = ${?HTTP.TIMEOUT}
  }

  mongo {

    database = "aibeyondComputeCluster"

    //uri = "mongodb://<username>:<password>@<hostname>:<port>/"${mongo.database}
    uri = "mongodb://localhost:27017"

    num-worker-agents = 5

    compute-agent-jobs {
      collection = "compute-jobs"
    }
  }

  # Settings for connecting and producting messages over Kafka
  kafka {
    num-producer-agents = 5

    bootstrap.servers = "kafka-1:19092,kafka-2:29092,kafka-3:39092"
    bootstrap.servers = ${?KAFKA.SERVERS}

    # The producer groups together any records that arrive in between request transmissions into a single
    #batched request. Normally this occurs only under load when records arrive faster than they can be
    #sent out. However in some circumstances the client may want to reduce the number of requests even
    #under moderate load. This setting accomplishes this by adding a small amount of artificial delayâ€”that
    #is, rather than immediately sending out a record the producer will wait for up to the given delay to
    #allow other records to be sent so that the sends can be batched together.
    # Increase in order to introduce delay and batch more messages together, 0 sends msg immediately
    linger.ms = 1

    # The number of acknowledgments the producer requires the leader to have received before
    #considering a request complete. This controls the durability of records that are sent.
    acks = 0

    # The total bytes of memory the producer can use to buffer records waiting to be sent to the server.
    # If records are sent faster than they can be delivered to the server the producer will block for
    # max.block.ms after which it will throw an exception.
    buffer.memory = 33554432

    # The configuration controls how long KafkaProducer.send() and KafkaProducer.partitionsFor() will block.
    # These methods can be blocked either because the buffer is full or metadata unavailable.Blocking in the
    # user-supplied serializers or partitioner will not be counted against this timeout.
    max.block.ms = 60000
  }
}

akka {
  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs to STDOUT)
  #loggers = ["akka.event.Logging$DefaultLogger"]
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "INFO" # used when normal logging ("loggers") has been started

  # You should also define akka.event.slf4j.Slf4jLoggingFilter in the logging-filter configuration property.
  # It will filter the log events using the backend configuration (e.g. logback.xml) before they are published
  # to the event bus
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off

  log-dead-letters-during-shutdown = off

  extensions = [
    "akka.cluster.metrics.ClusterMetricsExtension"
  ]

  coordinated-shutdown {
    exit-jvm = on
  }

  # Dealing with config specific to individual Actors
  actor {

    provider = "akka.cluster.ClusterActorRefProvider"

    serializers = {
      aira_serializer = "ai.beyond.compute.serializers.aira.AiraAgentSerializer"
      compute_serializer = "ai.beyond.compute.serializers.sample.ComputeAgentSerializer"
    }

    serialization-bindings = {
      # Setup all our base Messages from each agent class type
      "ai.beyond.compute.agents.aira.AiraSampleOneAgent$Message" = aira_serializer
      "ai.beyond.compute.agents.sample.ComputeAgent$Message" = compute_serializer
    }

    # Set this to on to enable serialization-bindings defined in
    # additional-serialization-bindings. Those are by default not included
    # for backwards compatibility reasons. They are enabled by default if
    # akka.remote.artery.enabled=on.
    enable-additional-serialization-bindings = on

    # Disable default Java Serialization system wide
    # Java Serializier is really slow, inefficient and full of security holes
    allow-java-serialization = off

    debug {

      # enable function of LoggingReceive, which is to log any received message at DEBUG level
      receive = on

      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill etc.)
      autoreceive = on

      # enable DEBUG logging of actor lifecycle changes
      lifecycle = on

      # enable DEBUG logging of unhandled messages
      unhandled = on

      # enable DEBUG logging of all LoggingFSMs for events, transitions and timers
      fsm = on

    }

  }

  # Remoting capability config, each remote is a node in the cluster, remotely talking to each other
  remote {
    # If this is "on", Akka will log all outbound messages at DEBUG level,
    # if off then they are not logged
    log-sent-messages = off

    # If this is "on", Akka will log all inbound messages at DEBUG level,
    # if off then they are not logged
    log-received-messages = off

    # Sets the log granularity level at which Akka logs remoting events. This setting
    # can take the values OFF, ERROR, WARNING, INFO, DEBUG, or ON. For compatibility
    # reasons the setting "on" will default to "debug" level. Please note that the effective
    # logging level is still determined by the global logging level of the actor system:
    # for example debug level remoting events will be only logged if the system
    # is running with debug level logging.
    # Failures to deserialize received messages also fall under this flag.
    log-remote-lifecycle-events = off

    # Logging of message types with payload size in bytes larger than
    # this value. Maximum detected size per message type is logged once,
    # with an increase threshold of 10%.
    # By default this feature is turned off. Activate it by setting the property to
    # a value in bytes, such as 1000b. Note that for all messages larger than this
    # limit there will be extra performance and scalability cost.
    log-frame-size-exceeding = 1000b

    ### Configuration for the Netty based transport drivers
    # netty.tcp {
    #  # The hostname or ip akka remoting should bind to
    #  hostname = "127.0.0.1"
    #  hostname = ${?AKKA.HOST}

    #  # The port akka remoting should bind to
    #  # Default is 2551 (AKKA), use 0 if you want a random available port
    #  # This port needs to be unique for each actor system on the same machine.
    #  port = 2551
    #  port = ${?AKKA.PORT}

    #  # Enables SSL support on this transport
    #  enable-ssl = false
    # }

    artery {
      enabled = on

      # See Selecting a transport at the link below
      # https://doc.akka.io/docs/akka/2.5/remoting-artery.html#selecting-a-transport
      # Options:
      # aeron-udp - Based on Aeron (UDP) - fastest but no tls support
      # tcp - Based on Akka Streams TCP - still fast supports tls but not enabled
      # tls-tcp - Same as tcp with encryption w/ Akka Streams TLS - needs setup with enc keys
      transport = aeron-udp # used for systems that require high throughput and low latency

      canonical.hostname = "127.0.0.1"
      canonical.hostname = ${?AKKA.HOST}

      canonical.port = 2551
      canonical.port = ${?AKKA.PORT}
    }
  }

  cluster {

    jmx.enabled = off
    metrics.enabled = off

    seed-nodes = [
      "akka://"${application.cluster.name}"@"${application.cluster.seed.host}":"${application.cluster.seed.port}
    ]

    # auto downing is NOT safe for production deployments.
    # you may want to use it during development, read more about it in the docs.
    #
    auto-down-unreachable-after = off

    sharding {
      # Cluster-Sharding Default Configuration Settings
      # https://doc.akka.io/docs/akka/2.5/cluster-sharding.html#configuration

      # Specifies that entities runs on cluster nodes with a specific role.
      # If the role is not specified (or empty) all nodes in the cluster are used.
      role = ""
      role = ${?CLUSTER.SHARDING.ROLE}

      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      # If this is 'off', the entity actors will not be automatically restarted due to
      # Shard restarts or node failures. They will be restarted when a message is intended for them
      # https://doc.akka.io/docs/akka/2.5/cluster-sharding.html#remembering-entities
      remember-entities = off

      # If the shard is remembering entities and an entity stops itself without
      # using passivate. The entity will be restarted after this duration or when
      # the next message for it is received, which ever occurs first.
      entity-restart-backoff = 10 s

      # Set this to a time duration to have sharding passivate entities when they have not
      # gotten any message in this long time. Set to 'off' to disable. enable i.e. '5 s'
      passivate-idle-entity-after = off

      # Defines how the coordinator stores its state. Same is also used by the
      # shards for rememberEntities.
      # Valid values are "ddata" or "persistence".
      state-store-mode = ddata
    }
  }

  # Persistence configuration
  # https://doc.akka.io/docs/akka/2.5/general/configuration.html#config-akka-persistence
  persistence {
    # journal plugin will use cassandra backend
    journal.plugin = "cassandra-journal"
    # List of journal plugins to start automatically. Use "" for the default journal plugin.
    journal.auto-start-journals = ["cassandra-journal"]

    # same with snapshots, which take incremental snapshots of incremental agents
    snapshot-store.plugin = "cassandra-snapshot-store"
    # List of snapshot stores to start automatically. Use "" for the default snapshot store.
    journal.auto-start-snapshot-stores = ["cassandra-snapshot-store"]
  }
}

# This configures the settings for all Cassandra Journal plugin
# https://github.com/akka/akka-persistence-cassandra/blob/master/core/src/main/resources/reference.conf
cassandra-journal {
  # FQCN of the cassandra journal plugin
  #class = "akka.persistence.cassandra.journal.CassandraJournal"

  # List of contact points in the Cassandra cluster.
  # Host:Port pairs are also supported. In that case the port parameter will be ignored.
  # The value can be either a proper list, e.g. ["127.0.0.1", "127.0.0.2"],
  # or a comma-separated list within a single string, e.g. "127.0.0.1,127.0.0.2".
  contact-points = ["127.0.0.1"]

  # Port of contact points in the Cassandra cluster.
  # Will be ignored if the contact point list is defined by host:port pairs.
  port = 9042

  # The identifier that will be passed as parameter to the
  # ConfigSessionProvider.lookupContactPoints method.
  cluster-id = "akka-persistence"

  # Name of the keyspace to be created/used by the journal
  keyspace = "akka" # Should match the CQL wherever Cassandra is started

  # Parameter indicating whether the journal keyspace should be auto created.
  # Not all Cassandra settings are configurable when using autocreate and for
  # full control of the keyspace and table definitions you should create them
  # manually (with a script).
  keyspace-autocreate = true

  # Parameter indicating whether the journal tables should be auto created
  # Not all Cassandra settings are configurable when using autocreate and for
  # full control of the keyspace and table definitions you should create them
  # manually (with a script).
  tables-autocreate = true
}

cassandra-snapshot-store {
  # FQCN of the cassandra snapshot store plugin
  #class = "akka.persistence.cassandra.snapshot.CassandraSnapshotStore"

  # Comma-separated list of contact points in the Cassandra cluster.
  # Host:Port pairs are also supported. In that case the port parameter will be ignored.
  contact-points = ["127.0.0.1"]

  # Port of contact points in the Cassandra cluster.
  # Will be ignored if the contact point list is defined by host:port pairs.
  port = 9042

  # The identifier that will be passed as parameter to the
  # ConfigSessionProvider.lookupContactPoints method.
  cluster-id = "akka-persistence"

  # Name of the keyspace to be created/used by the snapshot store
  keyspace = "akka_snapshot"

  # Parameter indicating whether the snapshot keyspace should be auto created
  # Not all Cassandra settings are configurable when using autocreate and for
  # full control of the keyspace and table definitions you should create them
  # manually (with a script).
  keyspace-autocreate = true

  # Parameter indicating whether the snapshot tables should be auto created
  # Not all Cassandra settings are configurable when using autocreate and for
  # full control of the keyspace and table definitions you should create them
  # manually (with a script).
  tables-autocreate = true

}
